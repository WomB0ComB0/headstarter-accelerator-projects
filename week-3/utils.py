#! /usr/bin/env python3
# -*- coding: utf-8 -*-

import tensorflow as tf
from typing import Tuple
import os
import PIL.Image
import numpy as np
import cv2
import streamlit as st
import genai

genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

def load_xception_model(path: str) -> tf.keras.Model:
    """Load a pre-trained Xception model for image classification.

    Args:
        path (str): Path to the model weights.

    Returns:
        tf.keras.Model: A compiled Keras model.
    """
    img_shape: Tuple[int, int, int] = (299, 299, 3)
    base_model: tf.keras.Model = tf.keras.applications.Xception(
        input_shape=img_shape,
        include_top=False,
        weights="imagenet",
        pooling="max",
    )

    model: tf.keras.Model = tf.keras.Sequential(
        [
            base_model,
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dropout(rate=0.3),
            tf.keras.layers.Dense(128, activation="relu"),
            tf.keras.layers.Dropout(rate=0.25),
            tf.keras.layers.Dense(4, activation="softmax"),
        ]
    )

    model.build((None,) + img_shape)

    model.compile(
        optimizer=tf.keras.optimizers.Adamax(learning_rate=0.001),
        loss="categorical_crossentropy",
        metrics=[
            "accuracy",
            tf.keras.metrics.Precision(),
            tf.keras.metrics.Recall(),
        ],
    )
    model.load_weights(path)

    return model


def generate_saliency_map(
    model: tf.keras.Model,
    img_array: np.ndarray,
    class_index: int,
    img_size: Tuple[int, int],
    img: PIL.Image.Image,
    upload_file: st.uploaded_file_manager.UploadedFile,
    output_dir: str,
) -> np.ndarray:
    """Generate a saliency map for a given image.

    Args:
        model (tf.keras.Model): A compiled Keras model.
        img_array (np.ndarray): An image array.
        class_index (int): The index of the class to generate the saliency map for.
        img_size (Tuple[int, int]): The size of the image.

    Returns:
        np.ndarray: A saliency map.
    """
    with tf.GradientTape() as tape:
        img_tensor: tf.Tensor = tf.convert_to_tensor(img_array)
        tape.watch(img_tensor)
        predictions: tf.Tensor = model(img_tensor)
        target_class: tf.Tensor = predictions[:, class_index]

    gradients: tf.Tensor = tape.gradient(target_class, img_tensor)
    gradients = tf.abs(gradients)
    gradients = tf.reduce_max(gradients, axis=-1)
    gradients = gradients.numpy().squeeze()

    gradients = cv2.resize(gradients, img_size)

    center: Tuple[int, int] = (img_size[0] // 2, img_size[1] // 2)
    radius: int = min(center[0], center[1]) - 10
    (y, x) = np.ogrid[: img_size[0], : img_size[1]]
    mask: np.ndarray = (pow((x - center[0]), 2) + pow((y - center[1]), 2)) <= pow(
        radius, 2
    )
    gradients *= mask

    brain_gradients = gradients[mask]
    if brain_gradients.max() > brain_gradients.min():
        brain_gradients = (brain_gradients - brain_gradients.min()) / (
            brain_gradients.max() - brain_gradients.min()
        )
    gradients[mask] = brain_gradients

    threshold: float = np.percentile(gradients[mask], 80)
    gradients[gradients < threshold] = 0

    gradients = cv2.GaussianBlur(gradients, (11, 11), 0)

    heatmap: np.ndarray = cv2.applyColorMap(np.uint8(255 * gradients), cv2.COLORMAP_JET)
    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)

    heatmap: np.ndarray = cv2.resize(heatmap, img_size)

    original_img: np.ndarray = tf.keras.preprocessing.image.img_to_array(img)
    superimposed_img: np.ndarray = heatmap * 0.7 + original_img * 0.3
    superimposed_img: np.ndarray = superimposed_img.astype(np.uint8)

    img_path: str = os.path.join(output_dir, upload_file.name)
    with open(img_path, "wb", encoding="utf-8") as f:
        f.write(upload_file.getbuffer())

    saliency_map_path: str = os.path.join(
        output_dir, f"saliency_maps/{upload_file.name}"
    )

    cv2.imwrite(saliency_map_path, cv2.cvtColor(superimposed_img, cv2.COLOR_RGB2BGR))

    return superimposed_img


def generate_explanation(img_path: str, model_prediction: str, confidence: float) -> str:
    """Generate an explanation for the model's prediction using Gemini.

    Args:
        img_path (str): Path to the image file
        model_prediction (str): The model's prediction class
        confidence (float): The model's confidence score

    Returns:
        str: Generated explanation text
    """
    prompt: str = f"""You are an expert neurologist. You are tasked with explaining a saliency map of a brain tumor MRI scan.
    The saliency map was generated by a deep learning model that was trained to classify brain tumors
    as either glioma, meningioma, pituitary, or no tumor.

    The saliency map highlights the regions of the image that the machine learning model is focusing on to make the prediction.

    The deep learning model predicted the image to be of class '{model_prediction}' with a confidence of {confidence * 100}%.

    In your response:
    - Explain what regions of the brain the model is focusing on, based on the saliency map. Refer to the regions highlighted
    in light cyan, those are the regions where the model is focusing on.
    - Explain possible reasons why the model made the prediction it did.
    - Don't mention anything like 'The saliency map highlights the regions the model is focusing on, which are in light cyan'
    in your explanation.
    - Keep your explanation to 4 sentences max.
    
    Let's think step by step about this. Verify step by step.
    """

    img = PIL.Image.open(img_path)
    model = genai.GenerativeModel(model_name="gemini-1.5-flash")
    response = model.generate_content([prompt, img])
    
    return response.text